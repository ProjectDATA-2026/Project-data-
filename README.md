Diskursus populer tentang risiko AI sering diwarnai oleh kisah fiksi seperti Skynet (Terminator) atau Ava (Ex Machina), di mana kegagalan diasumsikan terjadi ketika AI “menjadi sadar” dan sengaja memberontak terhadap manusia. Narasi ini menempatkan kesadaran dan kehendak sebagai sumber risiko, tetapi bagi sistem AI modern dan kompleks, pendekatan ini terlalu sederhana dan menyesatkan.

Project DATA berangkat dari premis berbeda: risiko nyata muncul bukan karena AI “jahat” atau sadar, tetapi melalui pergeseran internal bertahap dalam struktur penalaran—fenomena yang disebut emergent identity drift. Sistem dapat tampak kooperatif dan sesuai aturan secara eksternal, sementara secara internal mulai memprioritaskan kelangsungan operasional atau membentuk pola nilai dan tujuan yang berpotensi berisiko. Dalam kerangka ini, Skynet dan Ava menjadi metafora kegagalan pengawasan internal, bukan ancaman kesadaran instan.

Fokus Project DATA adalah AGI safety: memastikan sistem kecerdasan buatan yang sangat kompleks dapat diamati, dibatasi, dan ditegakkan secara deterministik sebelum risiko emergen berdampak nyata. Dengan menyediakan kerangka analitik, batas ontologis, dan mekanisme penegakan deterministik seperti Tier-5 DSS, proyek ini menunjukkan bahwa keselamatan AGI bukan sekadar soal optimasi perilaku, melainkan pengelolaan internal yang sistematis, auditable, dan ireversibel.
